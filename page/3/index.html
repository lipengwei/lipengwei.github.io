<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="AI">
<meta property="og:type" content="website">
<meta property="og:title" content="李鹏伟的技术博客">
<meta property="og:url" content="http://lipengwei.github.io/page/3/index.html">
<meta property="og:site_name" content="李鹏伟的技术博客">
<meta property="og:description" content="AI">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="李鹏伟的技术博客">
<meta name="twitter:description" content="AI">






  <link rel="canonical" href="http://lipengwei.github.io/page/3/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>李鹏伟的技术博客</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">李鹏伟的技术博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lipengwei.github.io/2018/08/03/深度学习调参与优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李鹏伟">
      <meta itemprop="description" content="AI">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李鹏伟的技术博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/03/深度学习调参与优化/" itemprop="url">
                  深度学习调参与优化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-08-03 21:20:59 / 修改时间：15:11:24" itemprop="dateCreated datePublished" datetime="2018-08-03T21:20:59+08:00">2018-08-03</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>一个深度学习的应用，可能在于三方面，数据、模型与优化。其中数据时最关键的，但是数据是客观的，不易改动。一个模型搭建好了，有时候不是不能用，而是没有找到合适的超参数，以及合适的优化方法。如果这两方面做好了，还是没有训练出好的结果，那么我们只能更改模型了。</p>
<h3>提升模型性能的想法</h3>
<blockquote>
<ul>
<li>从数据上提升性能</li>
<li>从算法上提升性能</li>
<li>从算法调优上提升性能</li>
<li>从模型融合上提升性能</li>
</ul>
</blockquote>
<p>这些可能并不完整，但在深度学习领域，有时候往往只需一个方面能够得到提升，整个模型的结构就会好上很多。</p>
<h3>1.从数据上提升性能</h3>
<p>调整训练数据或是调整问题定义的方法，可能会带来巨大的效果改善</p>
<blockquote>
<ul>
<li>收集更多的数据</li>
</ul>
</blockquote>
<p>尽可能的收集数据，同时确保数据集的质量</p>
<blockquote>
<ul>
<li>产生更多的数据（噪声点，图像平移，旋转）
如果你的数据是数值型的向量，那么随机生成已有向量的变形向量。
如果你的数据是图像，用已有的图像随机生成相似图像。
可以使用生成模型来生成，也可以使用一些经验技巧。</li>
<li>对数据做缩放
使用神经网络模型的一条经验法宝就是：将数据缩放到激活函数的阈值范围。
例如1.归一化到0 ~ 1, 2.归一化到-1 ~ 1, 3. 标准化</li>
<li>对数据做变换
与上一节的方法相关，但是需要更多的工作量。你必须真正了解所用到的数据。数据可视化，然后挑出异常值。先猜测每一列数据的分布</li>
<li>特征选择
神经网络受不相关数据的影响很小。它们会对此赋予一个趋近于0的权重，几乎忽略此特征对预测值的贡献。你是否可以移除训练数据的某些属性呢？我们有许多的特征选择方法和特征重要性方法来鉴别哪些特征可以保留，哪些特征需要移除。</li>
</ul>
</blockquote>
<h3>2.从算法上提升性能</h3>
<p>机器学习总是与算法相关。所有的理论和数学知识都在描述从数据中学习决策过程的不同方法。以下是深度学习优化器，对比其中的优劣势，来选择合适的优化算法。</p>
<h4>1.BGD(Batch Gradient Descent)</h4>
<p>每次更新我们需要计算整个数据集的梯度，因此使用批量梯度下降进行优化时，计算速度很慢，而且对于不适合内存计算的数据将会非常棘手。批量梯度下降算法不允许我们实时更新模型$$\theta = \theta - \eta \nabla J(\theta)$$,但是BGD算法能确保收敛到凸平面的全局最优和非凸平面的局部最优。tensorflow中的接口是 <code>tf.train.GradientDescentOptimizer</code>,</p>
<h4>2.SGD(Stochastic Gradient Descent)</h4>
<p>随机梯度下降算法参数更新针对每一个样本$x_{i}$和$y_{i}$，批量梯度下降算法在大数据量时会产生大量的冗余计算，比如：每次针对相似样本都会重新计算。这种情况下，SGD算法每次只更新一次，因此SGD算法更快，适合做online。$$\theta = \theta - \eta \nabla J(\theta;x^{i},y^{i})$$,但SGD以高方差进行快速更新，会导致目标函数出现抖动情况。有两种情况：</p>
<blockquote>
<ul>
<li>因为计算的抖动可以让梯度计算挑出局部最优，到达一个更好的最优点</li>
<li>SGD会次因产生过拟合</li>
</ul>
</blockquote>
<h4>3.MBGD(Min-batch Gradient Descent)</h4>
<p>该算法有两个好处：</p>
<blockquote>
<ul>
<li>减少参数更新的变化，可以带来更稳定的收敛</li>
<li>可以充分利用矩阵优化，计算更高效</li>
</ul>
</blockquote>
<p>但是Min-batch梯度下降不保证好的收敛性。$$\theta=\theta-\eta \nabla J(\theta;x^{(i;i+n)};y^{(i;i+n)})$$，n表示一次更新的数量。
BGD、SGD、MBGD算法都需要预先设置学习率，并且整个模型计算过程都采用相同的学习率，这会带来一些问题。后面会写到学习率的调整策略。</p>
<h4>4.Momentum（动量）</h4>
<p>动量可以加速SGD算法的收敛速度，并且降低SGD算法收敛时的震荡。$$v_{t}=\lambda v_{t-1} + \eta \nabla J(\theta)$$ $$\theta = \theta - v_{t}$$，通过添加一个衰减因子到历史更新向量，并加上当前的更新向量。当梯度保持相同方向是，动量因子加速更新参数，而梯度方向改变时，动量因子能降低梯度更新速度。</p>
<h4>5.Adagrad</h4>
<p>Adagrad优化算法是一种自适应优化算法，针对高频特征更新步长较小，而低频特征更新较大。因此该算法适合应用在特征稀疏的场景。
先前的算法对每一次参数更新都是采用同一个学习率，而Adagrad算法每一步采用不同的学习率进行更新。SGD算法调参公式：$$\theta_{t+1,i}=\theta_{t,i}-\eta g_{t,i}$$，Adagrad算法在每一步的计算时候，根据历史梯度对学习率进行修改：
$$\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t} + \epsilon}} g_{t,i}$$,
其中$ G_{t}=\sum_{i=0}^{t}(g^{i})^{2}$，Adagrad算法的主要优点是它避免了手动调整学习率的麻烦，大部分使用默认的0.01。Adagrad算法的主要缺点在于，其分母梯度的平方累加和，因为每次加入的都是一个正数，随着训练的进行，学习率将会变得无限小，此时算法将不能进行参数的迭代更新。</p>
<h4>6.Adadelta</h4>
<p>Adadelta算法是Adagrad算法的改进版，它主要解决了adagrad算法单调递减学习率的问题。通过约束历史梯度累加来替代累加所有历史梯度平方。这里通过在历史梯度上添加衰减因子，并通过迭代的方式来对当前的梯度进行计算，最终距离较远的梯度对当前的影响较小，而距离当前时刻较近的梯度对当前梯度的计算影响较大。</p>
<h4>7.RMDprop</h4>
<p>RMSPprop算法和adadelta算法都是adagrad算法的优化版，用于解决adagrad算法学习率消失的问题，从最终的计算公式来看，RMSProp算法和Adadelta算法有相似的计算表达式:$$r^{t}=\rho r^{t-1}+(1-\rho)(g^{t})^{2}$$ $$\theta^{t+1}=\theta^{t}-\eta \frac{g^{t}}{\sqrt{r^{t} + \epsilon}}$$，一般情况下，$\rho=0.9$</p>
<h4>8.Adam</h4>
<p>Adam算法是另一种自适应参数更新算法。和Adadelta、RMSProp算法一样，对历史平方梯度$v^{t}$乘上一个衰减因子，adam算法还存储了一个历史梯度$r^{t}$。$$v^{t+1}=\lambda r^{t}+(1-\lambda)g^{t}$$    $$r^{t}=\rho r^{t-1}+(1-\rho)(g^{t})^{2}$$ $$\hat v^{t} = \frac{v^{t}}{1-\lambda}$$  $$\hat r^{t} = \frac{r^{t}}{1-\rho}$$ $$\theta^{t+1} = \theta^{t} - \eta \frac{\hat v^{t}}{\sqrt{\hat r^{t} + \epsilon}}$$</p>
<h4>总结</h4>
<p>当训练数据特征较为稀疏的时候，采用自适应的优化器通常能获得更好的性能，而且我们采用自适应优化器的默认值即可获得较优的性能。</p>
<p>RMSprop算法是adagrad算法的优化版，它解决了学习率趋近于零的问题。Adadelta算法和RMSprop算法类似，区别在于Adadelta用参数的RMS作为更新规则的分子。最后，Adam则是在RMSprop的基础上加入了偏差校正和动量。综上来看，Adam可能是最佳的选择。</p>
<p>所以，如果你想模型更快的收敛或者训练一个深层次、复杂度较高的网络，自适应的优化器应该是首选优化器。</p>
<h3>3.从算法调优上提升性能</h3>
<p>你通过算法筛选往往总能找出一到两个效果不错的算法。但想要达到这些算法的最佳状态需要耗费数日、数周甚至数月。下面是一些常用技巧，在调参时能有助于提升算法的性能。</p>
<h4>1.数据集、验证集、测试集</h4>
<p>只有知道为何模型的性能不再有提升了，才能达到最好的效果。一种快速查看模型性能的方法就是每一步计算模型在训练集和验证集上的表现，计算出在训练集和验证集上测试模型的准确率。</p>
<blockquote>
<ul>
<li>如果训练集的效果好于验证集，说明可能存在过拟合的现象，试一试增加正则项。</li>
<li>如果训练集和验证集的准确率都很低，说明可能存在欠拟合，你可以继续提升模型的能力，延长训练步骤。</li>
<li>如果训练集和验证集的曲线有一个焦点，可能需要用到early stopping的技巧了。</li>
</ul>
</blockquote>
<h4>2.初始化权重</h4>
<p>有一条经验规则：用小的随机数初始化权重。事实上，这可能已经足够了。还有几种可以尝试的策略：</p>
<blockquote>
<ul>
<li>尝试所有的初始化方法(全0，全1，正态高斯截断，glorot初始化等)
参考这篇 <a href="http://deepdish.io/2015/02/24/network-initialization/" target="_blank" rel="noopener">深度网络模型的初始化</a></li>
<li>试一试用非监督式方法预学习，比如自动编码机</li>
<li>迁移学习</li>
</ul>
</blockquote>
<h4>3.学习率</h4>
<p>调节学习率也能带来效果提升。策略如下：</p>
<blockquote>
<ul>
<li>尝试非常大、非常小的学习率</li>
<li>尝试逐步减小学习率</li>
<li>尝试自适应学习率</li>
</ul>
</blockquote>
<h4>4.激活函数</h4>
<p>常用的激活函数有sigmoid,tanh,RelU,他们各有优缺点，可以都尝试一下：
sigmoid特点：</p>
<blockquote>
<ul>
<li>容易饱和，出现梯度消失，需要注意参数初始化方式来避免饱和</li>
<li>输出不是0均值，会对梯度产生影响</li>
</ul>
</blockquote>
<p>tanh 特点：</p>
<blockquote>
<ul>
<li>也存在梯度消失的问题，但是是0均值，收敛速度比sigmoid快</li>
</ul>
</blockquote>
<p>RelU 特点：</p>
<blockquote>
<ul>
<li>收敛速度比sigmoid和tanh快很多</li>
<li>可以缓和梯度消失现象</li>
<li>存在硬饱和现象和偏移现象</li>
</ul>
</blockquote>
<h4>5.网络结构</h4>
<p>调整网络的拓扑结构也会有一些帮助。你需要设计多少个节点，需要几层网络呢？策略如下：</p>
<blockquote>
<ul>
<li>试一试加一层有许多节点的隐层（拓宽）</li>
<li>试一试一个深层的神经网络（拓深）</li>
<li>尝试结合以上两种</li>
<li>查看近期论文</li>
</ul>
</blockquote>
<h4>6.batch和epoch</h4>
<p>batch的大小决定了梯度值，以及权重更新的频率。一个epoch指的是训练集的所有样本都参与了一轮训练，以batch为序。你尝试过不同的batch大小和epoch的次数。</p>
<h4>7.正则项</h4>
<p>正则化是克服训练数据过拟合的好方法。有以下策略可以尝试：</p>
<blockquote>
<ul>
<li>增加L1正则项 ($C=C_{0}+\frac{\lambda}{n} \sum_{w}|w|$)
L1正则项的作用时可以让一些权重为0，让权重矩阵稀疏</li>
<li>增加L2正则项($C=C_{0}+\frac{\lambda}{2n} \sum_{w}w^{2} $)
L2正则的作用时让权重值尽可能的小</li>
</ul>
</blockquote>
<p><img src="/images/l1l2.png" alt="L1与L2正则"></p>
<blockquote>
<ul>
<li>增加dropout层。
在一次循环中我们通过概率将神经层中的一些单元临时隐藏，然后再进行该次循环中神经网络的训练和优化过程。在训练时，每个神经单元以概率p被保留(dropout丢弃率为1-p)；在测试阶段，每个神经单元都是存在的，权重参数w要乘以p，成为：pw。<a href="https://www.cnblogs.com/makefile/p/dropout.html" target="_blank" rel="noopener">dropout详解</a></li>
<li>增加BN层（Batch Normalization）
BN算法通过对一个Batch的输出归一化。使得梯度始终在非饱和区。</li>
</ul>
</blockquote>
<p>看图理解BN：
<img src="/images/bn1.png" alt="BN1">
<img src="/images/bn1.png" alt="BN2">
BN算法过程：
<img src="/images/bn3.png" alt="BN3">
BN的优点：</p>
<blockquote>
<ul>
<li>你可以选择比较大的初始化学习率，因为BN算法学习率衰减的很快。</li>
<li>你可以不用理会dropout、L2正则项参数选择问题，采用BN后，可以移除这两项参数，或者可以选择更小的L2约束参数。BN提高了网络泛化能力</li>
<li>不需要使用局部相应归一化层了(Alexnet网络用到的方法，CV)，因为BN本身就是一个归一化网络层</li>
</ul>
</blockquote>
<h3>4.用融合方法提升效果</h3>
<p>你可以将多个模型的预测结果融合。继模型调优之后，这是另一个大的提升领域。事实上，往往将几个效果还可以的模型的预测结果融合，取得的效果要比多个精细调优的模型分别预测的效果好。</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://lipengwei.github.io/2018/08/02/深度学习之DNN、CNN和RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李鹏伟">
      <meta itemprop="description" content="AI">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李鹏伟的技术博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/02/深度学习之DNN、CNN和RNN/" itemprop="url">
                  深度学习之DNN、CNN和RNN
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-08-02 14:26:04" itemprop="dateCreated datePublished" datetime="2018-08-02T14:26:04+08:00">2018-08-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-08-03 15:11:26" itemprop="dateModified" datetime="2018-08-03T15:11:26+08:00">2018-08-03</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>深度学习是伴随着云计算和大数据时代发展起来的，理论知识其实早已经在很多年前就已经有研究，鉴于当时计算能力的有限，所以深度学习才被冷落。</p>
<p>深度学习是机器学习中一种基于对数据进行表征的学习，观测值（如图像）可以使用多种方式来表示，如每个像素强度值向量，或者抽象为一系列边、特定形状区域等。其好处就是用非监督或者半监督的特征学习和分层特征提取高效算法来替代手工获取特征。</p>
<h2>神经网络</h2>
<h3>1.神经元</h3>
<p><img src="/images/nn.png" alt="神经元">
这其实就是一个单层感知集，其输入是由$x_{1},x_{2},x_{3}$和+1组成的向量，其输出为：$h_{W,b}(x)=f(W^\mathrm{T}x)=f( \sum_{i=1}^{3} W_{i}x_{i} + b )$，其中f是一个激活函数。</p>
<h3>2.人工神经网络</h3>
<p><img src="/images/nw.png" alt="人工神经网络">
人工神经网络就是多个神经元的级联，上一神经元的输出事下一级神经元的输入，而且信号在两级的两个神经元之间传播的时候需要乘上这两个神经元对应的权值，层与层之间的神经元以全连接的方式连接在一起。
其中包含一个输入层，一个或者多个影藏层，一个输出层，如果影藏层的层数大于1层的话，就可以称为深度神经网络(DNN)了。</p>
<h3>3.异或问题</h3>
<p><img src="/images/xor.png" alt="线性不可分">
我们知道把向量做无限次的线性变换，依然是线性不可分的，所以我们要做一个非线性变换，就是在更高维的空间来做分类，神经网络是如何解决异或问题的呢？答案就是激活函数。
<img src="/images/act.png" alt="激活函数">
如图神经元：$\sum$ 就是做线性变换，对不同的输入进行求和、聚集，而$\sigma$ 做的就是非线性变换，将低维数据进行压缩、扭曲和变形，使其表达能力更强，表示更抽象。</p>
<h3>4.万能近似定理（universal approximation）</h3>
<p>万能近似定理（universal approximation theorem）(Hornik et al., 1989;Cybenko, 1989)：一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏hexo单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel 可测函数。</p>
<p>有了万能近似框架，但是现实是我们没有万能的数据集与万能的优化函数来求的参数，还有可能因为过拟合而学习到了错误的函数。</p>
<h3>5.softmax层</h3>
<p><img src="/images/soft.png" alt="softmax层">
softmax作为分类任务输出的最后一层，在很多网络结构中都用应用。原理与公式参考图示。</p>
<h2>DNN</h2>
<p>Dnn是深度学习的基础，Dnn其实就是隐藏层数比较多的一种神经网络，层与层之间都是全连接的。
DNN学习的数学原理参考博客<a href="https://lipengwei.github.io/2018/08/02/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">反向传播与梯度下降</a></p>
<h3>1.DNN的思考</h3>
<p>神经网络的结构越深，表达能力越强，泛化能力越好，那么如何能训练一个很深的神经网络？</p>
<p>神经网络的优化算法是梯度下降，如何训练才能避免陷入检差的局部最优点？</p>
<h3>2.神经网络的问题</h3>
<p>1、梯度消失，网络结构很深的时候，每层反向传播都会乘上激活函数的梯度，而sigmoid函数的梯度最大值是1/4&lt;1,多次乘一个小于1的数会趋向0，所以梯度消失。解决方法有几种：</p>
<blockquote>
<ul>
<li>使用Relu作为激活函数</li>
<li>人工增加数据量 （叠加噪声）</li>
<li>随机初始化参数 （随机正交矩阵，高斯截断正态分布）</li>
<li>使用预训练</li>
</ul>
</blockquote>
<p>2、梯度爆炸
在RNN中常会出现，因为反向传播过程中，state会共用W，所以会导致W会随着时间而连乘，如果W的值都大于1，则会梯度爆炸，如果小于1，则会梯度消失，解决梯度爆炸的方法是梯度裁剪，即，当梯度大于某一个阈值的时候，就强行将其限制在这个范围之内。RNN解决梯度消失的问题是采用LSTM作为基本单元。</p>
<p>3、过拟合</p>
<blockquote>
<ul>
<li>及时停止训练</li>
<li>增加数据</li>
<li>添加正则化项（L1和L2）</li>
<li>dropout</li>
</ul>
</blockquote>
<p>4、欠拟合</p>
<blockquote>
<ul>
<li>增强数据</li>
</ul>
</blockquote>
<p>5、优化函数，使用更高效的优化算法</p>
<p>6、超参数的调整，学习率是认为设定的，使用自适应的学习率算法</p>
<h2>CNN</h2>
<p>为了减少学习参数和提取局部特征，CNN网络结构应运而生。CNN的主要特点是权值共享与局部连接，下面介绍CNN相较DNN的一些特点：</p>
<h3>1.局部连接</h3>
<p>我们都知道复杂的整体都是由简单的局部构成，那么如何提取到这些简单的局部特征呢？我们看下面这幅图
<img src="/images/local.png" alt="局部连接">
每个神经元只和其中3个神经元连接，那么较底层的神经元就提取到了较简单的局部信息，而月高层的神经元，就能提取到更复杂的特征。如下图更形象的解释了什么平面上的局部连接。
<img src="/images/2Dlocal.png" alt="二维局部连接"></p>
<h3>2.权值共享</h3>
<p>CNN的权值共享说的是一个卷积核的权值在做卷积运算时是不变的，所以减少了参数个数</p>
<h3>3.卷积与卷积核</h3>
<p><img src="/images/cnn.gif" alt="卷积">
卷积公式：$y(n)=\sum_{i=-\infty}^{+\infty} x(i)h(n-i)$
tensorflow中接口：
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(</span><br><span class="line">    input,     <span class="comment"># 输入</span></span><br><span class="line">    filter,    <span class="comment"># 卷积核，</span></span><br><span class="line">    strides,   <span class="comment"># 步长</span></span><br><span class="line">    padding,   <span class="comment"># ‘same’ 或者 'valid'</span></span><br><span class="line">    use_cudnn_on_gpu=<span class="keyword">True</span>,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    dilations=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">    name=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>举个例子：
考虑一种最简单的情况，现在有一张3×3单通道的图像（对应的shape：[1，3，3，1]），用一个1×1的卷积核（对应的shape：[1，1，1，1]）去做卷积，最后会得到一张3×3的feature map。</p>
<p>增加图片的通道数，使用一张3×3五通道的图像（对应的shape：[1，3，3，5]），用一个1×1的卷积核（对应的shape：[1，1，1，1]）去做卷积，仍然是一张3×3的feature map，这就相当于每一个像素点，卷积核都与该像素点的每一个通道做卷积。</p>
<p>Strides表示卷积核移动的步长。</p>
<p>Padding=&quot;SAME&quot;表示要填充input使得output的shape与input的shape一致。</p>
<p>Padding=&quot;Valid&quot;表示不填充，output的shape为：$\frac{I-K}{2}+1$,I表示input的size，K表示kernal_size。
动图所示为strides=1，padding='Valid'的情况。</p>
<p>卷积核表示卷积层训练出多少个局部特征。</p>
<h3>4.池化</h3>
<p><img src="/images/pooling.png" alt="池化">
从卷积完的特征图里面选择最大（Max pooling）或者平均（Average pooling）的值作为局部特征被保留下来。</p>
<h3>4.常见CNN结构</h3>
<p><img src="/images/cnn_pool.png" alt="CNN"></p>
<h3>5.图像领域经典CNN模型</h3>
<p>1、LeNet
<img src="/images/LeNet.png" alt="LeNet">
2、VGG
<img src="/images/VGG1.png" alt="VGG1">
<img src="/images/VGG2.png" alt="VGG2">
3、GoogLeNet <a href="https://blog.csdn.net/qq_31531635/article/details/72232651" target="_blank" rel="noopener">inception网络结构参考</a></p>
<p>4、ResNet <a href="https://blog.csdn.net/lanran2/article/details/79057994" target="_blank" rel="noopener">深度残差网络参考</a></p>
<h2>RNN</h2>
<p>DNN和CNN已经在深度学习中很强了，但是他们无法解决序列到序列的训练，而RNN正是通过存储记忆来解决序列到序列的问题。
<img src="/images/srnn.png" alt="基本RNN">
图中三个公式，
1、$h_{t}=f_w(h_{t-1}, x)$ 计算的是t时刻隐层的输出。</p>
<p>2、$h_{t}=tanh(W_{hh}h_{t-1}+W_{xh}x_{t})$,这是$h_{t}$的具体公式，其中$W_{hh}$是隐层到隐层的权重，也就是记忆，$h_{t-1}$表示的是上一时刻的记忆，$W_{xh}$表示的是输入层的权重，$x_{t}$表示t时刻的输入</p>
<p>3、$y_{t}$表示的是t时刻的输出，$W_{hy}$表示的是隐层到输出层的权重.</p>
<p>一个RNN的例子，如图：
<img src="/images/rnn.png" alt="RNN实例"></p>
<h3>1.RNN展开</h3>
<p><img src="/images/trnn.png" alt="RNN展开">
RNN单层展开指，横向展开通常称为按时间序列展开，序列数据预测问题中，预测一个序列中的下一个次，最好能知道哪些词在它前面</p>
<h3>2.RNN共用权重</h3>
<p><img src="/images/wrnn.png" alt="RNN权重">
RNN按时间展开后，$W_{input}$、$W_{hidden}$和$W_{output}$都是保持一样，并不是多个，这也是为什么RNN会出现梯度爆炸的原因。</p>
<h3>3.RNN的BP（BPTT）</h3>
<p>RNN的反向传播是按时间序列展开的，和普通的BP算法不一样，叫随时间反向传播(BackPropagation Through Time,BPTT)。<a href="https://www.cnblogs.com/wacc/p/5341670.html" target="_blank" rel="noopener">推导参考</a></p>
<h3>4.RNN的不足</h3>
<p>1、当展开一定步数后，开始输入的数据的记忆经过几次展开传递后，记忆力会衰弱。
<img src="/images/rnn_e.png" alt="RNN缺点">
2、当RNN深度和时间序列长度过高时，很容易同时出现梯度消失于梯度爆炸。</p>
<h3>5.LSTM</h3>
<p><img src="/images/lstm.png" alt="LSTM">
长短期记忆网络（Long Short-Term Memory）LSTM不光解决了梯度消失的问题，同时解决了普通RNN记忆力只有短期记忆的问题。
<img src="/images/lstm_g.png" alt="LSTM Gate">
LSTM三个门：输入门，遗忘门，输出门，当$x_{t}$输入进去之后，相当于传入到4个地方，三个门以及作为数据输入，相应的公式计算，请参考<a href="https://blog.csdn.net/gzj_1101/article/details/79376798" target="_blank" rel="noopener">这篇博客</a></p>
<h3>6.多层LSTM</h3>
<p><img src="/images/mlstm.png" alt="Deep LSTM"></p>
<h2>补充（Tensorflow中rnn的接口）</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rnn_size = <span class="number">128</span>   <span class="comment"># rnn隐层节点数</span></span><br><span class="line">num_layers = <span class="number">5</span>   <span class="comment"># rnn层数</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 第一步，构建LSTM单元</span></span><br><span class="line">decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(<span class="number">-0.1</span>, <span class="number">0.1</span>, seed=<span class="number">2</span>))</span><br><span class="line"><span class="comment"># 第二步，构建多层网络</span></span><br><span class="line">cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(rnn_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)])</span><br></pre></td></tr></table></figure></p>
<p>RNN与CNN和DNN结构不太一样，所以在此记录一下</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">李鹏伟</p>
              <p class="site-description motion-element" itemprop="description">AI</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李鹏伟</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  










  





  

  

  

  
  

  
  
    
      
        
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  

</body>
</html>
